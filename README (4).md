# Data Modelling Project with Postgres

## Context

A startup called Sparkify wants to analyze their overall user activity on their new music streaming app, specifically their song data. Currently, they don't have an easy way to query their data and perform analysis on it. The data currently lives in a  directory of JSON logs on user activity on the app, along with a directory with JSON metadata on the songs in their app. Therefore it's necessary to create a database that is able to handle the requested analytical reports.

## Relational Database Schema Choice and ETL Process
### The ideal relational database schema for this context is the Star Schema because:

    -The star schema is optimized for reads and aggregations
    -The star schema allows users to find answers with actionable insights by performing ad-hoc analysis
    -The data source files' structure and data types are predictable
    -The star schema can be scalable as the user base grows and the streaming app generates more data
    -The traditional operational database schema (transactional schema) which is used in the applications back end is only optimized for inserts and updates 
    
    
### An ETL pipeline is necessary because:    
    -The original data is taken directly from the app thus it is presented in a raw format (json format) which makes it difficult to analyze
    -It allows for the startup to have a predictable way of preparing and transforming the raw data into an analyst-friendly format 
    
    
## Data Sources:  

#### Song datasets: 
    - Subset of real data from the Million Song Dataset
    - These these json files are nested in subdirectories under */data/song_data*.
    - Example of json record:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```        

#### Log datasets:
    - Generated by an event simulator based on the songs in the dataset above
    -These json files are nested in subdirectories under */data/log_data*.
    -Example of json record:
```
{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}
```   

## Database Schema    
    
### 1 Fact table with measures associated with each song play:
        -songplays (records with page NextSong)
        -songplay_id (SERIAL) PRIMARY KEY 
        -start_time (DATE) NOT NULL
        -user_id (INT) NOT NULL
        -level (TEXT) NOT NULL (plan type)
        -song_id (INT) NULL
        -artist_id (VARCHAR) NOT NULL
        -session_id (INT) NOT NULL
        -location (TEXT) (User location)
        -user_agent (TEXT) NOT NULL (Method of access to the platform)
### 4 dimensional tables with metadata about the artists and songs:
    - Users table:
        -user_id (INT) PRIMARY KEY
        -first_name (TEXT) NOT NULL
        -last_name (TEXT) NOT NULL
        -gender (TEXT) NOT NULL
        -level (TEXT) NOT NULL (plan type)
    - Songs table:
        -song_id (INT) NULL
        -title (TEXT) NOT NULL
        -artist_id (VARCHAR) NOT NULL
        -year (INT) NOT NULL
        -duration (FLOAT) NOT NULL
    - Artists table:
        -artist_id (VARCHAR) PRIMARY KEY
        -name (TEXT) NOT NULL
        -location (TEXT) NOT NULL
        -latitude (FLOAT) NOT NULL
        longitude (FLOAT) NOT NULL
    - Time table:
        -start_time (DATE) PRIMARY KEY
        -hour (INT)
        -day (INT)
        -week (INT)
        -month (INT)
        -year (INT)
        -weekday (TEXT)
        
        
## Instructions to Run Python Scripts

In order to create the database and fill it out with data through the ETL pipeline, it is necessary to run through the console two files containing the Python scripts in the following sequence:

To create tables:
```bash
python create_tables.py
```
To fill out tables with data via ETL processes:
```bash
python etl.py